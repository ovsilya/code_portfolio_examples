{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import cross_validation\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] = ['Tahoma','DejaVu Sans','Lucida Grande','Verdana']\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_params = [bigquery.ScalarQueryParameter(\"day_of_activity\", \"INT64\", 10),\n",
    "               bigquery.ScalarQueryParameter(\"sub_plan_date\", \"STRING\", \"2018-11-22\"),\n",
    "               bigquery.ScalarQueryParameter(\"dayforsub_check\", \"INT64\", 31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "codeCollapsed": false,
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "def execute_sql_queries_to_build_traindata(query_parameters):\n",
    "  query_params = query_parameters\n",
    "  \n",
    "  sql_all_data = \"\"\" SELECT teacher_id, days_since_account_creation, day_of_activity, date, account_created_at, total_students, \n",
    "  xp, hp, random_event, random_picker, message, discussion_comment, power_usage, gear_equipping, gear_item_purchase, \n",
    "  per_training, level_up, DATE_ADD(date, INTERVAL @dayforsub_check DAY) AS date_for_subscription_check\n",
    "  FROM(\n",
    "  SELECT teacher_id, plan,days_since_account_creation,day_of_activity,date,trial,total_students, date(account_created_at) as account_created_at, xp, hp, random_event, random_picker, message, objective_completion, objective_creation, quest_creation, quest_importation, discussion_comment, assignment_feedback, power_usage, gear_equipping, gear_item_purchase, per_training, level_up\n",
    "  FROM(\n",
    "  SELECT *, days_since_account_creation + 1 - ( SELECT MIN(days_since_account_creation)\n",
    "                                                FROM `classcraft-dev.Ilya_OVS_dataset.account_upgrades_no_weekends`\n",
    "                                                WHERE teacher_id = a.teacher_id\n",
    "                                                GROUP BY teacher_id) as day_of_activity,\n",
    "    CASE \n",
    "      WHEN date > '2018-04-30' THEN 'yes' \n",
    "      ELSE 'no'\n",
    "    END logbook_data\n",
    "  FROM `classcraft-dev.Ilya_OVS_dataset.account_upgrades_no_weekends` AS a\n",
    "  WHERE DATE(account_created_at) BETWEEN '2018-08-15' AND '2018-10-31'\n",
    "    AND teacher_id NOT IN (\n",
    "      SELECT teacher_id \n",
    "      FROM `classcraft-dev.Ilya_OVS_dataset.account_upgrades_no_weekends` \n",
    "      WHERE days_since_account_creation = 0 AND plan <> 'freemium'))\n",
    "  WHERE logbook_data = 'yes' and day_of_activity <= @day_of_activity)\n",
    "  \"\"\"\n",
    "  ##################################################################################\n",
    "  job_config = bigquery.QueryJobConfig()\n",
    "  job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "  table_ref = client.dataset(\"Ilya_OVS_dataset\").table('prediction_pipeline_backup_table_1')\n",
    "  job_config.destination = table_ref\n",
    "  job_config.query_parameters = query_params\n",
    "  job_config.use_standard_sql = True\n",
    "\n",
    "  df_all_data = client.query(sql_all_data, job_config=job_config)\n",
    "  df_all_data.result()\n",
    "  # print('Query results loaded to table {}'.format(table_ref.path))\n",
    "  ##################################################################################\n",
    "  ##################################################################################\n",
    "  query_2 = \"\"\" SELECT A.teacher_id, A.total_students, EXTRACT(DAYOFYEAR FROM A.account_created_at) AS account_created_dayofyear, \n",
    "  A.xp, A.hp, A.random_event, A.random_picker, A.message, A.discussion_comment, A.power_usage, A.gear_equipping, A.gear_item_purchase,\n",
    "  A.per_training, A.level_up,\n",
    "\n",
    "  CASE\n",
    "    WHEN B.plan IS NULL THEN \"freemium\"\n",
    "    ELSE B.plan END AS label\n",
    "\n",
    "  FROM(\n",
    "  SELECT \n",
    "  teacher_id,\n",
    "  MIN(total_students) AS  total_students, \n",
    "  MIN(account_created_at) AS  account_created_at,\n",
    "  SUM(xp) AS  xp, \n",
    "  SUM(hp) AS  hp, \n",
    "  SUM(random_event) AS  random_event,\n",
    "  SUM(random_picker) AS  random_picker,\n",
    "  SUM(message) AS  message,\n",
    "  SUM(discussion_comment) AS  discussion_comment, \n",
    "\n",
    "  SUM(power_usage) AS power_usage,\n",
    "  SUM(gear_equipping) AS gear_equipping,\n",
    "  SUM(gear_item_purchase) AS gear_item_purchase,\n",
    "  SUM(per_training) AS per_training,\n",
    "  SUM(level_up) AS level_up,\n",
    "\n",
    "  MIN(date_for_subscription_check) AS  date_for_subscription_check\n",
    "\n",
    "  FROM `classcraft-dev.Ilya_OVS_dataset.prediction_pipeline_backup_table_1`\n",
    "  GROUP BY teacher_id) AS A\n",
    "  LEFT JOIN `classcraft-dev.game_activities.subscription_status_log` as B\n",
    "  ON A.teacher_id = B.teacher_id and A.date_for_subscription_check = B.date\n",
    "\n",
    "  \"\"\"\n",
    "  ##################################################################################\n",
    "  job_config_2 = bigquery.QueryJobConfig()\n",
    "  table_ref_2 = client.dataset(\"Ilya_OVS_dataset\").table('prediction_pipeline_backup_table_2')\n",
    "  job_config_2.write_disposition = \"WRITE_TRUNCATE\"\n",
    "  job_config_2.destination = table_ref_2\n",
    "  job_config_2.query_parameters = query_params\n",
    "  job_config_2.use_standard_sql = True\n",
    "\n",
    "  df_train_data = client.query(query_2, job_config=job_config_2)\n",
    "  df_train_data.result()\n",
    "  # print('Query results loaded to table {}'.format(table_ref_2.path))\n",
    "  \n",
    "execute_sql_queries_to_build_traindata(query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of freemium data entries:  (9002, 14)\n",
      "number of premium data entries:  (1493, 14)\n"
     ]
    }
   ],
   "source": [
    "sql_freemium = \"\"\"\n",
    "SELECT\n",
    "account_created_dayofyear, total_students, xp, hp, random_event, random_picker, message, power_usage, gear_equipping, gear_item_purchase, per_training, level_up, discussion_comment, CASE WHEN label = \"freemium\" THEN 0 ELSE 1 END AS label\n",
    "FROM `classcraft-dev.Ilya_OVS_dataset.prediction_pipeline_backup_table_2`\n",
    "WHERE label = \"freemium\" and label <> \"license\"  and label <> \"organization\"\n",
    "\"\"\"\n",
    "#######################\n",
    "sql_preemium = \"\"\"\n",
    "SELECT\n",
    "account_created_dayofyear, total_students, xp, hp, random_event, random_picker, message, power_usage, gear_equipping, gear_item_purchase, per_training, level_up, discussion_comment, CASE WHEN label = \"freemium\" THEN 0 ELSE 1 END AS label \n",
    "FROM `classcraft-dev.Ilya_OVS_dataset.prediction_pipeline_backup_table_2`\n",
    "WHERE label <> \"freemium\" and label <> \"license\"  and label <> \"organization\"\n",
    "\"\"\"\n",
    "\n",
    "df_freemium = client.query(sql_freemium).to_dataframe()\n",
    "df_preemium = client.query(sql_preemium).to_dataframe()\n",
    "print(\"number of freemium data entries: \", df_freemium.shape)\n",
    "print(\"number of premium data entries: \", df_preemium.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "num_features = df_freemium.shape[1] - 1\n",
    "dataset_freemium_downsampled = resample(df_freemium, \n",
    "                                 replace=False, # sample without replacement\n",
    "                                 n_samples=df_preemium.shape[0], # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "# Combine minority class with downsampled majority class\n",
    "dataset_balanced = pd.concat([dataset_freemium_downsampled, df_preemium])\n",
    "################ removing outliers ####################\n",
    "z = np.abs(stats.zscore(dataset_balanced))\n",
    "# print(z)\n",
    "dataset_balanced_outliers = dataset_balanced[(z < 3).all(axis=1)]\n",
    "#converting the balanced DataFrame into a Numpy Array:\n",
    "array_all_data_balanced = dataset_balanced_outliers.as_matrix()\n",
    "# splitting data into a X - observations and Y - labels\n",
    "\n",
    "X = array_all_data_balanced[:,0:num_features]\n",
    "Y = array_all_data_balanced[:,num_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account_created_dayofyear', 'total_students', 'xp', 'hp', 'random_event', 'random_picker', 'message', 'power_usage', 'gear_equipping', 'gear_item_purchase', 'per_training', 'level_up', 'discussion_comment', 'label'] \n",
      "\n",
      "total number of instances in balanced data BEFORE removing outliers:  2986\n",
      "total number of instances in balanced data AFTER removing outliers:  2618\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(dataset_balanced.columns[0:])\n",
    "# print (\"num_features: \", num_features)\n",
    "print(feature_names, \"\\n\")\n",
    "print (\"total number of instances in balanced data BEFORE removing outliers: \",dataset_balanced.shape[0])\n",
    "print (\"total number of instances in balanced data AFTER removing outliers: \",dataset_balanced_outliers.shape[0])\n",
    "# print(\"\\n\\n\")\n",
    "# print(dataset_balanced.describe())\n",
    "# print(\"\\n\\n\")\n",
    "# print(dataset_balanced_outliers.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 23.136369466781616 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Data Standardization - Standardize features by removing the mean and scaling to unit variance\n",
    "# Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set.\n",
    "# Standardization of a dataset is a common requirement for many machine learning estimators\n",
    "# In the interest of preventing information about the distribution of the test set leaking into your model, \n",
    "# we fit the scaler on training data only, then standardise both training and test sets with that scaler.\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "model = XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
    "                      min_child_weight=1, missing=None, n_estimators=100, nthread=-1, objective='binary:logistic', reg_alpha=0, \n",
    "                      reg_lambda=1, scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
    "\n",
    "######################### StandardScaler() #########################\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_transformed = scaler.fit_transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train_transformed, y_train)\n",
    "predictions = model.predict(X_test_transformed)\n",
    "\n",
    "######################### RobustScaler() #########################\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "X_train_transformed_robust = transformer.transform(X_train)\n",
    "X_test_transformed_robust = transformer.transform(X_test)\n",
    "\n",
    "\n",
    "model.fit(X_train_transformed_robust, y_train)\n",
    "predictions_robust = model.predict(X_test_transformed_robust)\n",
    "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.08%\n",
      "Cross-validation score: 89.94% (+/- 0.02%)\n",
      "Confusion matrix for XGBoost classifier:\n",
      "[[260  21]\n",
      " [ 31 212]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions for XGBoost\n",
    "xgb_accuracy = accuracy_score(y_test, predictions_robust)\n",
    "print(\"Accuracy: %.2f%%\" % (xgb_accuracy * 100.0))\n",
    "\n",
    "\n",
    "# evaluate predictions for XGBoost with cross-validation score:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=7)\n",
    "scores = cross_val_score(model, X, Y, cv=cv)\n",
    "print(\"Cross-validation score: %.2f%% (+/- %0.2f%%)\" % (scores.mean()*100.0, scores.std() * 2))\n",
    "\n",
    "\n",
    "# confusion matrix for XGBoost clasifier\n",
    "XGBoost_conf_matrix = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion matrix for XGBoost classifier:\")\n",
    "print (XGBoost_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hiddenCell": true
   },
   "outputs": [],
   "source": [
    "# boxplot zisualizing outliers \n",
    "# plt.figure(figsize=(20, 8))\n",
    "# sns.boxplot(data = X_train_transformed_robust)\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=X_train_transformed[12])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['hp'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['random_event'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['random_picker'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['message'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['power_usage'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['gear_equipping'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['gear_item_purchase'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['per_training'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['level_up'])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(30, 15))\n",
    "# sns.boxplot(x=dataset_balanced['discussion_comment'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
